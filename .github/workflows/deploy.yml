name: PySpark Job CI/CD Pipeline

on:
  push:
    branches:
      - main

jobs:
  build-and-push:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v2

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v1

    - name: Set up QEMU
      uses: docker/setup-qemu-action@v1

    - name: Set up AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

    - name: Log in to Amazon ECR
      id: ecr-login
      run: |
        aws ecr get-login-password --region ${{ secrets.AWS_REGION }} | docker \
        login --username AWS --password-stdin ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ secrets.AWS_REGION }}.amazonaws.com

    - name: Build and push Docker image
      env:
        ECR_REGISTRY: 005010853392.dkr.ecr.us-east-1.amazonaws.com
        ECR_REPOSITORY: spark_job_container
        IMAGE_TAG: latest

      run: |
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG

  deploy:
    runs-on: ubuntu-latest
    needs: build-and-push

    steps:
    - name: Set up environment variables
    - run: |
        echo "CLUSTER_ID=${{ secrets.EMR_CLUSTER_ID }}" >> $GITHUB_ENV

    - name: Run PySpark job on EMR
      run: |
        aws emr add-steps --cluster-id $CLUSTER_ID \
        --steps Type=CUSTOM_JAR,Name="RunContainerizedSparkJob",ActionOnFailure=CONTINUE,Jar=command-runner.jar,\
        Args=["spark-submit","--master","yarn","--deploy-mode","cluster","--conf","spark.executor.instances=2",\
        "--conf","spark.kubernetes.container.image=005010853392.dkr.ecr.us-east-1.amazonaws.com/spark_job_container:latest",\
        "local:///app/emr_spark_job.py"]
        
